{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "SNAKE_LEN_GOAL = 30\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-500, high=500, shape=(5 + SNAKE_LEN_GOAL,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.prev_actions.append(action)\n",
    "        cv2.imshow(\"a\", self.img)\n",
    "        cv2.waitKey(1)\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "\n",
    "        # Takes step after fixed time\n",
    "        t_end = time.time() + 0.05\n",
    "        k = -1\n",
    "        while time.time() < t_end:\n",
    "            if k == -1:\n",
    "                k = cv2.waitKey(1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        button_direction = action\n",
    "        # Change the head position based on the button direction\n",
    "        if button_direction == 1:\n",
    "            self.snake_head[0] += 10\n",
    "        elif button_direction == 0:\n",
    "            self.snake_head[0] -= 10\n",
    "        elif button_direction == 2:\n",
    "            self.snake_head[1] += 10\n",
    "        elif button_direction == 3:\n",
    "            self.snake_head[1] -= 10\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(\n",
    "                self.apple_position, self.score\n",
    "            )\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "\n",
    "        else:\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "\n",
    "        # On collision kill the snake and print the score\n",
    "        if (\n",
    "            collision_with_boundaries(self.snake_head) == 1\n",
    "            or collision_with_self(self.snake_position) == 1\n",
    "        ):\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "            cv2.putText(\n",
    "                self.img,\n",
    "                \"Your Score is {}\".format(self.score),\n",
    "                (140, 250),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            cv2.imshow(\"a\", self.img)\n",
    "            self.done = True\n",
    "\n",
    "        self.total_reward = len(self.snake_position) - 3  # default length is 3\n",
    "        self.reward = self.total_reward - self.prev_reward\n",
    "        self.prev_reward = self.total_reward\n",
    "\n",
    "        if self.done:\n",
    "            self.reward = -1\n",
    "        info = {}\n",
    "\n",
    "        head_x = self.snake_head[0]\n",
    "        head_y = self.snake_head[1]\n",
    "\n",
    "        snake_length = len(self.snake_position)\n",
    "        apple_delta_x = self.apple_position[0] - head_x\n",
    "        apple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "        # create observation:\n",
    "\n",
    "        observation = [\n",
    "            head_x,\n",
    "            head_y,\n",
    "            apple_delta_x,\n",
    "            apple_delta_y,\n",
    "            snake_length,\n",
    "        ] + list(self.prev_actions)\n",
    "        observation = np.array(observation)\n",
    "        truncated = False\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation, self.reward, self.done, truncated, info\n",
    "\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "        self.score = 0\n",
    "        self.prev_button_direction = 1\n",
    "        self.button_direction = 1\n",
    "        self.snake_head = [250, 250]\n",
    "\n",
    "        self.prev_reward = 0\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "        head_x = self.snake_head[0]\n",
    "        head_y = self.snake_head[1]\n",
    "\n",
    "        snake_length = len(self.snake_position)\n",
    "        apple_delta_x = self.apple_position[0] - head_x\n",
    "        apple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "        self.prev_actions = deque(\n",
    "            maxlen=SNAKE_LEN_GOAL\n",
    "        )  # however long we aspire the snake to be\n",
    "        for i in range(SNAKE_LEN_GOAL):\n",
    "            self.prev_actions.append(-1)  # to create history\n",
    "\n",
    "        # create observation:\n",
    "        observation = [\n",
    "            head_x,\n",
    "            head_y,\n",
    "            apple_delta_x,\n",
    "            apple_delta_y,\n",
    "            snake_length,\n",
    "        ] + list(self.prev_actions)\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SnekEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "while True:\n",
    "    check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnekEnv()\n",
    "episodes = 50\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while True:  # not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        print(\"action\", random_action)\n",
    "        obs, reward, done, _, info = env.step(random_action)\n",
    "        print(\"reward\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env = SnekEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 0\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 3\n",
      "reward -1\n",
      "action 2\n",
      "reward -1\n",
      "action 1\n",
      "reward -1\n",
      "action 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m random_action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_action)\n\u001b[0;32m---> 10\u001b[0m obs, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(random_action)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m, in \u001b[0;36mSnekEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m<\u001b[39m t_end:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m         k \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = SnekEnv()\n",
    "episodes = 50\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while True:  # not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        print(\"action\", random_action)\n",
    "        obs, reward, done, _, info = env.step(random_action)\n",
    "        print(\"reward\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
